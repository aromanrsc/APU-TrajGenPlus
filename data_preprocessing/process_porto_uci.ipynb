{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porto trajectory processing\n",
    "\n",
    "The current notebook work with the [Taxi Service Trajectory - Prediction Challenge, ECML PKDD 2015](https://archive.ics.uci.edu/dataset/339/taxi+service+trajectory+prediction+challenge+ecml+pkdd+2015) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import pickle\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset file. The original train.csv is around 1.8GB. To create smaller files you can use the following command to extract lines from train.csv:\n",
    "# $head -n50000 train.csv > train_50000.csv\n",
    "READ_PATH = \"../original_datasets/Porto-UCI/train.csv/train_last_100000.csv\"\n",
    "\n",
    "# Path where to pickle original data\n",
    "WRITE_PATH_ORG = \"../datasets/Porto/porto_uci_31k_org_drop_only.pkl\"\n",
    "\n",
    "# Path where to pickle trajectories\n",
    "WRITE_PATH_TRAJ = \"../datasets/Porto/porto_uci_31k_traj_drop_only.pkl\"\n",
    "\n",
    "# Path where to pickle trajectories\n",
    "WRITE_PATH_TRAJ_TIME_DIFF = \"../datasets/Porto/porto_uci_31k_traj_time_diff.pkl\"\n",
    "\n",
    "# Path where to pickle trajectories\n",
    "WRITE_PATH_TRAJ_ANALYSIS = \"../datasets/Porto/porto_uci_analysis.pkl\"\n",
    "\n",
    "\n",
    "# Path where to pickle trajectories\n",
    "WRITE_PATH_TRAJ_ANALYSIS_60k = \"../datasets/Porto/porto_uci_analysis_60k.pkl\"\n",
    "\n",
    "# Columns of the original dataset (train.csv)\n",
    "columns = [\"TRIP_ID\",\"CALL_TYPE\",\"ORIGIN_CALL\",\"ORIGIN_STAND\",\"TAXI_ID\",\"TIMESTAMP\",\"DAY_TYPE\",\"MISSING_DATA\",\"POLYLINE\"]\n",
    "\n",
    "# Columns not needed in each row of the dataset\n",
    "columns_to_drop = ['CALL_TYPE', 'ORIGIN_CALL', \"ORIGIN_STAND\", 'DAY_TYPE']\n",
    "\n",
    "# Chunk size parameter for the pandas' read_csv() function.\n",
    "# See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "chunk_size = 10000\n",
    "\n",
    "# CSV file delimiter for columns\n",
    "delimiter = \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTH_RADIUS_KM = 6371\n",
    "EARTH_RADIUS_M  = 6371008.7714\n",
    "\n",
    "# Minimum number of points in a trajectory. A trajectory with less then MIN_TRAJECTORY_POINTS is dropped\n",
    "MIN_TRAJECTORY_POINTS = 25 \n",
    "\n",
    "# Parameter to filter trajectories. Used to check if haversine distance between two points is under THRESHOLD_M\n",
    "THRESHOLD_M = 100\n",
    "\n",
    "# Parameter to filter trajectories. Allowed sampling time in seconds between trajectory points. Should be allowed 2x sampling rate.\n",
    "TIME_DIFF_THRESHOLD_S = 120\n",
    "\n",
    "# Parameter to filter trajectories. Maximum allowed speed between trajectory points in km.\n",
    "SPEED_KM_THRESHOLD = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(file_path, encoding=None):\n",
    "    \"\"\"\n",
    "    To be used after calling save_object().\n",
    "    Loads a python object that was pickled with save_object()\n",
    "\n",
    "    :param str file_path: Path to the pickled\n",
    "    :param str encoding: None or type of file encoding\n",
    "    :return: None if load fails, else the pickled object\n",
    "    :rtype: None if fails or type of objected pickled with save_object() \n",
    "    \"\"\"\n",
    "    \n",
    "    if file_path is not None:\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "\n",
    "            if encoding is not None:\n",
    "                return pickle.load(file, encoding='latin1')\n",
    "            else:\n",
    "                return pickle.load(file)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, file_path):\n",
    "    \"\"\"\n",
    "    Save python object obj in the given file_path with pickle.\n",
    "\n",
    "    :param object obj: Python object to be saved\n",
    "    :param str file_path: Path where to save obj\n",
    "    :return: If the save was successful\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "\n",
    "    if obj is not None and file_path is not None:\n",
    "\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(obj, file)\n",
    "\n",
    "        return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Compute great-circle distance between two lat/lon points in km.\n",
    "    \"\"\"\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    return haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * EARTH_RADIUS_KM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyline_as_list(val):\n",
    "    \"\"\"\n",
    "    The current dataset (train.csv) stores polylines as string, containing a list of lists. This function convert a str param val into a list of lists.\n",
    "    \"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if pd.isna(val) or val in [\"\", \"[]\"]:\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (Exception):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_datetime(unix_timestamp):\n",
    "    \"\"\"\n",
    "    Converts a unix timestamp into date and time.\n",
    "    \n",
    "    Example:\n",
    "        unix_time: 1744097710\n",
    "        gives\n",
    "        date: 2025-08-04\n",
    "        time: 07:35:10\n",
    "\n",
    "    :param int unix_timestamp: Unix time as int\n",
    "    :return: Tuple containing the date and time from unix_timestamp\n",
    "    :rtype: touple\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)\n",
    "\n",
    "    date = dt.strftime(\"%Y-%m-%d\")\n",
    "    time = dt.strftime('%H:%M:%S')\n",
    "\n",
    "    return date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to read large size file\n",
    "def csv_dataset_reader(file_path, chunk_size, columns, delimiter, dtype):\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, names=columns, delimiter=delimiter, dtype=dtype):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = csv_dataset_reader(READ_PATH, chunk_size=chunk_size, columns=columns, delimiter=delimiter, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_gps_file_batched(\n",
    "    file_path, chunk_size, columns, columns_to_drop,\n",
    "    delimiter, dtype, output_dir, batch_size=50000\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    reader = csv_dataset_reader(file_path, chunk_size, columns, delimiter, dtype)\n",
    "\n",
    "    batch = []\n",
    "    file_counter = 0\n",
    "    traj_counter = 0\n",
    "\n",
    "    for data_chunks in reader:\n",
    "        data_chunks.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "        for _, row in data_chunks.iterrows():\n",
    "            polyline = polyline_as_list(row.POLYLINE)\n",
    "            if len(polyline) < 25:\n",
    "                continue\n",
    "\n",
    "            lats = np.array([pt[1] for pt in polyline], dtype=np.float32)\n",
    "            lons = np.array([pt[0] for pt in polyline], dtype=np.float32)\n",
    "            n = len(lats)\n",
    "\n",
    "            timestamps = int(row[\"TIMESTAMP\"]) + np.arange(n) * 15\n",
    "\n",
    "            timestamps = timestamps.astype(np.int64)\n",
    "            time_diff = np.diff(timestamps, prepend=timestamps[0])\n",
    "\n",
    "            lat_next = np.roll(lats, 1)\n",
    "            lon_next = np.roll(lons, 1)\n",
    "            lat_next[0] = np.nan\n",
    "            lon_next[0] = np.nan\n",
    "\n",
    "            distances = np.full(n, np.nan, dtype=np.float32)\n",
    "            speeds = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "            for i in range(1, n):\n",
    "                distances[i] = haversine_distance_km(lats[i], lons[i], lat_next[i], lon_next[i])\n",
    "                speeds[i] = distances[i] / (time_diff[i] / 3600) if time_diff[i] > 0 else 0\n",
    "\n",
    "            timestamps_dt = [timestamp_to_datetime(ts) for ts in timestamps]\n",
    "            date_time_df = pd.DataFrame(timestamps_dt, columns=[\"date\", \"time\"])\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                \"lat\": lats,\n",
    "                \"lon\": lons,\n",
    "                \"timestamp\": timestamps,\n",
    "                \"time_diff\": time_diff,\n",
    "                \"distance_km\": distances,\n",
    "                \"speed_km\": speeds,\n",
    "                \"missing\": row[\"MISSING_DATA\"],\n",
    "                \"trajectory_id\": traj_counter\n",
    "            })\n",
    "\n",
    "            df = pd.concat([df, date_time_df], axis=1)\n",
    "\n",
    "            batch.append(df)\n",
    "            traj_counter += 1\n",
    "\n",
    "            if len(batch) >= batch_size:\n",
    "                save_object(batch, f\"{output_dir}/trajectories_{file_counter:03d}.pkl\")\n",
    "                print(f\"Saved file trajectories_{file_counter:03d}.pkl with {len(batch)} trajectories\")\n",
    "                batch.clear()\n",
    "                file_counter += 1\n",
    "\n",
    "    # Write remaining data\n",
    "    if batch:\n",
    "        save_object(batch, f\"{output_dir}/trajectories_{file_counter:03d}.pkl\")\n",
    "        print(f\"Saved file trajectories_{file_counter:03d}.pkl with {len(batch)} trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file trajectories_000.pkl with 50000 trajectories\n",
      "Saved file trajectories_001.pkl with 50000 trajectories\n",
      "Saved file trajectories_002.pkl with 50000 trajectories\n",
      "Saved file trajectories_003.pkl with 50000 trajectories\n",
      "Saved file trajectories_004.pkl with 50000 trajectories\n",
      "Saved file trajectories_005.pkl with 50000 trajectories\n",
      "Saved file trajectories_006.pkl with 50000 trajectories\n",
      "Saved file trajectories_007.pkl with 50000 trajectories\n",
      "Saved file trajectories_008.pkl with 50000 trajectories\n",
      "Saved file trajectories_009.pkl with 50000 trajectories\n",
      "Saved file trajectories_010.pkl with 50000 trajectories\n",
      "Saved file trajectories_011.pkl with 50000 trajectories\n",
      "Saved file trajectories_012.pkl with 50000 trajectories\n",
      "Saved file trajectories_013.pkl with 50000 trajectories\n",
      "Saved file trajectories_014.pkl with 50000 trajectories\n",
      "Saved file trajectories_015.pkl with 50000 trajectories\n",
      "Saved file trajectories_016.pkl with 50000 trajectories\n",
      "Saved file trajectories_017.pkl with 50000 trajectories\n",
      "Saved file trajectories_018.pkl with 50000 trajectories\n",
      "Saved file trajectories_019.pkl with 50000 trajectories\n",
      "Saved file trajectories_020.pkl with 50000 trajectories\n",
      "Saved file trajectories_021.pkl with 50000 trajectories\n",
      "Saved file trajectories_022.pkl with 50000 trajectories\n",
      "Saved file trajectories_023.pkl with 50000 trajectories\n",
      "Saved file trajectories_024.pkl with 50000 trajectories\n",
      "Saved file trajectories_025.pkl with 50000 trajectories\n",
      "Saved file trajectories_026.pkl with 50000 trajectories\n",
      "Saved file trajectories_027.pkl with 36128 trajectories\n"
     ]
    }
   ],
   "source": [
    "process_large_gps_file_batched(file_path=\"../original_datasets/Porto-UCI/train.csv/train.csv\",\n",
    "                               chunk_size=chunk_size,\n",
    "                              columns=columns,\n",
    "                              columns_to_drop=columns_to_drop,\n",
    "                              delimiter=delimiter,\n",
    "                              dtype=object,\n",
    "                              output_dir=\"../datasets/Porto/all/\",\n",
    "                              batch_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_dataset = [df for df in dataset if len(df['lat']) >= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_under_threshold(df, threshold=100):\n",
    "    # Returns true if distance between max min lat is under threshold\n",
    "\n",
    "    max = [ df[\"lat\"].max(), df[\"lon\"].max()] \n",
    "    min = [ df[\"lat\"].min(), df[\"lon\"].min()] \n",
    "\n",
    "    max_in_radians = [radians(_) for _ in max]\n",
    "    min_in_radians = [radians(_) for _ in min]\n",
    "\n",
    "\n",
    "    distance_m = haversine_distances([max_in_radians, min_in_radians]) * 6371008.7714\n",
    "\n",
    "    if distance_m[0, 1] < threshold:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered: trajectories_000.pkl\n",
      "Saved filtered: trajectories_001.pkl\n",
      "Saved filtered: trajectories_002.pkl\n",
      "Saved filtered: trajectories_003.pkl\n",
      "Saved filtered: trajectories_004.pkl\n",
      "Saved filtered: trajectories_005.pkl\n",
      "Saved filtered: trajectories_006.pkl\n",
      "Saved filtered: trajectories_007.pkl\n",
      "Saved filtered: trajectories_008.pkl\n",
      "Saved filtered: trajectories_009.pkl\n",
      "Saved filtered: trajectories_010.pkl\n",
      "Saved filtered: trajectories_011.pkl\n",
      "Saved filtered: trajectories_012.pkl\n",
      "Saved filtered: trajectories_013.pkl\n",
      "Saved filtered: trajectories_014.pkl\n",
      "Saved filtered: trajectories_015.pkl\n",
      "Saved filtered: trajectories_016.pkl\n",
      "Saved filtered: trajectories_017.pkl\n",
      "Saved filtered: trajectories_018.pkl\n",
      "Saved filtered: trajectories_019.pkl\n",
      "Saved filtered: trajectories_020.pkl\n",
      "Saved filtered: trajectories_021.pkl\n",
      "Saved filtered: trajectories_022.pkl\n",
      "Saved filtered: trajectories_023.pkl\n",
      "Saved filtered: trajectories_024.pkl\n",
      "Saved filtered: trajectories_025.pkl\n",
      "Saved filtered: trajectories_026.pkl\n",
      "Saved filtered: trajectories_027.pkl\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"../datasets/Porto/all/\"\n",
    "output_dir = \"../datasets/Porto/all/filtered/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for fname in sorted(os.listdir(input_dir)):\n",
    "    \n",
    "    if fname.endswith(\".pkl\"):\n",
    "        \n",
    "        file_path = os.path.join(input_dir, fname)\n",
    "        dataset = load_object(file_path)\n",
    "        \n",
    "        if dataset is None:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if any((df['missing'] == True).any() for df in dataset):\n",
    "            print(f\"Skipping {fname} due to missing data.\")\n",
    "            continue\n",
    "        else:\n",
    "            filtered_trajectories = []\n",
    "\n",
    "            for traj in dataset:\n",
    "                \n",
    "                if (traj[\"speed_km\"] >= 150).any():\n",
    "                    continue\n",
    "\n",
    "                if trajectory_under_threshold(traj):\n",
    "                    continue\n",
    "\n",
    "                if not traj[\"lon\"].between(-9.0, -8.0).all() or not traj[\"lat\"].between(40.1, 41.7).all():\n",
    "                    continue\n",
    "\n",
    "                traj.drop(columns=[\"missing\"])\n",
    "\n",
    "                filtered_trajectories.append(traj)\n",
    "\n",
    "\n",
    "            save_object(dataset, os.path.join(output_dir, fname))\n",
    "            print(f\"Saved filtered: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trajectories = []\n",
    "\n",
    "for traj in filtered_dataset:\n",
    "    \n",
    "    if (traj[\"speed_km\"] >= 150).any():\n",
    "        continue\n",
    "\n",
    "    if trajectory_under_threshold(traj):\n",
    "        continue\n",
    "\n",
    "    if not traj[\"lon\"].between(-9.0, -8.0).all() or not traj[\"lat\"].between(40.1, 41.7).all():\n",
    "        continue\n",
    "\n",
    "    # filtered_trajectories.append(fix_latlon_spikes_sklearn(traj, threshold_meters=300))\n",
    "    filtered_trajectories.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72575"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trajectories_analisys = [df.drop(columns=[\"missing\"]) for df in filtered_trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_object(filtered_trajectories_analisys, \"../datasets/Porto/porto_uci_analysis_last_60k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trajectories_time_diff = [df.drop(columns=[\"timestamp\"]) for df in filtered_trajectories_analisys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_trajectories_time_diff = load_object(WRITE_PATH_TRAJ_TIME_DIFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat lon\n",
    "# save_time_diff = [df[[\"lat\", \"lon\", \"time_diff\", \"distance_km\", \"speed_km\"]] for df in filtered_trajectories_time_diff]\n",
    "save_object(filtered_trajectories_time_diff, WRITE_PATH_TRAJ_TIME_DIFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = [df.drop(columns=[\"time_diff\", \"timestamp\", \"distance_km\", \"speed_km\", \"missing\"]) for df in filtered_trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_object(dataset, WRITE_PATH_ORG)\n",
    "save_object(to_save, WRITE_PATH_TRAJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_object(WRITE_PATH_TRAJ_ANALYSIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "timestamp",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "time_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "distance_km",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "speed_km",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e33e8ecb-fe5d-4aab-80df-0099d5376c03",
       "rows": [
        [
         "0",
         "41.18049",
         "-8.645994",
         "1372637091",
         null,
         null,
         null
        ],
        [
         "1",
         "41.180517",
         "-8.645949",
         "1372637106",
         "15.0",
         "0.004816284189121449",
         "1.1559082053891478"
        ],
        [
         "2",
         "41.180049",
         "-8.646048",
         "1372637121",
         "15.0",
         "0.05269466044783471",
         "12.646718507480331"
        ],
        [
         "3",
         "41.178888",
         "-8.646804",
         "1372637136",
         "15.0",
         "0.14376805872790643",
         "34.50433409469755"
        ],
        [
         "4",
         "41.178465",
         "-8.649495",
         "1372637151",
         "15.0",
         "0.23007428199950672",
         "55.217827679881616"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>speed_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.180490</td>\n",
       "      <td>-8.645994</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.180517</td>\n",
       "      <td>-8.645949</td>\n",
       "      <td>1372637106</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>1.155908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.180049</td>\n",
       "      <td>-8.646048</td>\n",
       "      <td>1372637121</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.052695</td>\n",
       "      <td>12.646719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.178888</td>\n",
       "      <td>-8.646804</td>\n",
       "      <td>1372637136</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.143768</td>\n",
       "      <td>34.504334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.178465</td>\n",
       "      <td>-8.649495</td>\n",
       "      <td>1372637151</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.230074</td>\n",
       "      <td>55.217828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat       lon   timestamp  time_diff  distance_km   speed_km\n",
       "0  41.180490 -8.645994  1372637091        NaN          NaN        NaN\n",
       "1  41.180517 -8.645949  1372637106       15.0     0.004816   1.155908\n",
       "2  41.180049 -8.646048  1372637121       15.0     0.052695  12.646719\n",
       "3  41.178888 -8.646804  1372637136       15.0     0.143768  34.504334\n",
       "4  41.178465 -8.649495  1372637151       15.0     0.230074  55.217828"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
