{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f836939",
   "metadata": {},
   "source": [
    "#### Functions Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db654c2",
   "metadata": {},
   "source": [
    "##### **Module Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a59ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# oneDNN warning suppression TF 2.4.1\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import energy_distance, wasserstein_distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from math import radians\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee72ba",
   "metadata": {},
   "source": [
    "##### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe4066cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 837\n",
    "\n",
    "SEED = 42\n",
    "TESTING_SIZE = 0.2\n",
    "\n",
    "# Total number of trajectories. If set to 0, all trajectories are used\n",
    "TOTAL_TRAJS = 1000\n",
    "\n",
    "TRAINING_TESTING_SAME_FILE = True\n",
    "\n",
    "# SELECTED_DATASET = \"SANFRANCISCO\" \n",
    "SELECTED_DATASET = \"PORTO\"  \n",
    "\n",
    "\n",
    "DATASET = {\"PORTO\": \"datasets/Porto/porto_uci_31k_traj_drop_only.pkl\",\n",
    "           \"SANFRANCISCO\": \"datasets/San_Francisco/train_trajectories.pkl\"}\n",
    "\n",
    "TESTING_FILE = None\n",
    "\n",
    "COLUMNS = [\"lat\", \"lon\"]\n",
    "\n",
    "DATA_SQUARE_SF = { \n",
    "                \"lon_1\": 37.86499,\n",
    "                \"lat_1\": -122.53304,\n",
    "                \"lon_2\": 37.68481,\n",
    "                \"lat_2\": -122.30576\n",
    "                }\n",
    "\n",
    "DATA_SQUARE_PORTO = { \n",
    "                \"lon_1\": 41.23969,\n",
    "                \"lat_1\": -8.73005,\n",
    "                \"lon_2\": 41.05951,\n",
    "                \"lat_2\": -8.49195\n",
    "                }\n",
    "\n",
    "DATA_SQUARE = {\"SANFRANCISCO\": DATA_SQUARE_SF,\n",
    "               \"PORTO\": DATA_SQUARE_PORTO}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15025a",
   "metadata": {},
   "source": [
    "##### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "223ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELLS = 16 #32\n",
    "SEQ_LEN = 25 #25\n",
    "BATCH_SIZE = 16 #32\n",
    "EPOCHS = 100\n",
    "LR = 0.01\n",
    "\n",
    "STATEFUL = False\n",
    "RETURN_SEQ = True\n",
    "\n",
    "NUM_FEATS = 2\n",
    "NUM_OUTPUTS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d331c",
   "metadata": {},
   "source": [
    "##### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10dcb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(data, data_path):\n",
    "    \"\"\" Save data to a pickle file \"\"\"\n",
    "    \n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2472c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(data_path):\n",
    "    \"\"\" Load data from file path\"\"\"\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ec37b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(data_path, num_of_traj = 0):\n",
    "    \"\"\" Load data from pickle file \"\"\"\n",
    "    \n",
    "    df = load_pickle(data_path)\n",
    "    \n",
    "    if num_of_traj == 0:\n",
    "        num_of_traj = len(df)\n",
    "        \n",
    "    df = df[:num_of_traj]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74c4b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_Y_from_data(data, num_of_traj):\n",
    "    \"\"\"\n",
    "        Create X and Y from the data.\n",
    "        X is composed of the trajectory data starting from the first point to the second last point\n",
    "        Y is composed of the trajectory data starting from the second point to the last point\n",
    "        Keeps only the selected COLUMNS (defined in the parameters block)\n",
    "    \"\"\"\n",
    "    X, Y = [0.0]  * num_of_traj, [0.0] * num_of_traj\n",
    "\n",
    "\n",
    "    for i in range(num_of_traj):\n",
    "        # X is composed of the trajectory data starting from the first point to the second last point\n",
    "        X[i] =  data[i][COLUMNS].iloc[0:-1] \n",
    "        X[i].columns = COLUMNS\n",
    "\n",
    "        # Y is composed of the trajectory data starting from the second point to the last point\n",
    "        Y[i] =  data[i][COLUMNS].iloc[1:] \n",
    "        Y[i].columns = COLUMNS\n",
    "        \n",
    "        X[i] = X[i].to_numpy()\n",
    "        Y[i] = Y[i].to_numpy()\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de160d9",
   "metadata": {},
   "source": [
    "##### **Data preprocessing, Normalization and Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32799f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_in_square(data, square):\n",
    "    \"\"\" Get data inside the square defined by the square dictionary \"\"\" \n",
    "    \n",
    "    # Ensure correct bounds regardless of coordinate sign or order\n",
    "    lat_min = min(square[\"lat_1\"], square[\"lat_2\"])\n",
    "    lat_max = max(square[\"lat_1\"], square[\"lat_2\"])\n",
    "    lon_min = min(square[\"lon_1\"], square[\"lon_2\"])\n",
    "    lon_max = max(square[\"lon_1\"], square[\"lon_2\"])\n",
    "\n",
    "    filtered_data = []\n",
    "    \n",
    "    for traj in data:\n",
    "        in_lat_bounds = traj[\"lat\"].between(lat_min, lat_max)\n",
    "        in_lon_bounds = traj[\"lon\"].between(lon_min, lon_max)\n",
    "\n",
    "        if (in_lat_bounds & in_lon_bounds).all():\n",
    "            filtered_data.append(traj)\n",
    "            \n",
    "            \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd6d9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_from_data(data):\n",
    "    # Get trajectories min and max values\n",
    "    num_of_traj = len(data)\n",
    "    mins, maxs = [0.0] * num_of_traj, [0.0] * num_of_traj\n",
    "\n",
    "    for i in range(num_of_traj):\n",
    "        mins[i]  = np.array(data[i].min()) \n",
    "        maxs[i] = np.array(data[i].max()) \n",
    "\n",
    "    mins =  np.min( np.array(mins), axis = 0)[0 : 2]\n",
    "    maxs =  np.max( np.array(maxs), axis = 0)[0 : 2]\n",
    "    \n",
    "    return mins, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96321ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset, \n",
    "                   normalization_type = 'min-max',\n",
    "                   normalization_ranges = None,\n",
    "                   testing_data_norm = False):\n",
    "    \"\"\"\n",
    "        Function to normalize the dataset using either min-max or standard normalization.\n",
    "        Can be used for separate testing data normalization or for normalization of the whole dataset with other ranges.\n",
    "        Returns the normalized dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalization_type == 'min-max':   \n",
    "        scaler = MinMaxScaler()\n",
    "    elif normalization_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    # Normalize the dataset using the ranges given in normalization_ranges (min and max)        \n",
    "    # Used for separate testing data normalization or for normalization of the whole dataset with other ranges\n",
    "    \n",
    "    if normalization_ranges is not None:\n",
    "        scaler.min = normalization_ranges[\"min\"]\n",
    "        scaler.max = normalization_ranges[\"max\"]\n",
    "    else:\n",
    "        scaler.fit(dataset)\n",
    "        \n",
    "    columns = dataset.columns\n",
    "    \n",
    "    norm_dataset = scaler.transform(dataset)\n",
    "    norm_dataset = pd.DataFrame(norm_dataset, columns = columns)\n",
    "    \n",
    "    return scaler, norm_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "259e2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_trajectory_data(dataset, \n",
    "                   normalization_type = 'min-max',\n",
    "                   normalization_ranges = None,\n",
    "                   testing_data_norm = False,\n",
    "                   scaler = None):\n",
    "    \"\"\"\n",
    "        Function to normalize the dataset using either min-max or standard normalization.\n",
    "        Can be used for separate testing data normalization or for normalization of the whole dataset with other ranges.\n",
    "        Returns the normalized dataset.\n",
    "    \"\"\"\n",
    "    dataset_cpy = copy.deepcopy(dataset)\n",
    "    \n",
    "    if testing_data_norm is False:\n",
    "        if scaler is None:\n",
    "            if normalization_type == 'min-max':   \n",
    "                scaler = MinMaxScaler()\n",
    "            elif normalization_type == 'standard':\n",
    "                scaler = StandardScaler()\n",
    "        \n",
    "        # Normalize the dataset using the ranges given in normalization_ranges (min and max)        \n",
    "        # Used for separate testing data normalization or for normalization of the whole dataset with other ranges\n",
    "        \n",
    "        if normalization_ranges is not None:\n",
    "            X_min = normalization_ranges[\"min\"]\n",
    "            X_max = normalization_ranges[\"max\"]\n",
    "            dataset_cpy = [(arr - X_min) / (X_max - X_min) for arr in dataset_cpy]\n",
    "            \n",
    "        else:\n",
    "            dataset_flat = pd.concat(dataset_cpy, ignore_index=True)\n",
    "            # dataset_flat = np.vstack(dataset_cpy)\n",
    "            scaler.fit(dataset_flat)\n",
    "            \n",
    "            columns = dataset_cpy[0].columns\n",
    "            \n",
    "            for i in range(len(dataset_cpy)):\n",
    "                norm_dataset = scaler.transform(dataset_cpy[i])\n",
    "                norm_dataset = pd.DataFrame(norm_dataset, columns = columns)\n",
    "                dataset_cpy[i] = norm_dataset\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if normalization_ranges is not None:\n",
    "            X_min = normalization_ranges[\"min\"]\n",
    "            X_max = normalization_ranges[\"max\"]\n",
    "            dataset_cpy = [(arr - X_min) / (X_max - X_min) for arr in dataset_cpy]\n",
    "            \n",
    "        else:\n",
    "            columns = dataset_cpy[0].columns\n",
    "        \n",
    "            for i in range(len(dataset_cpy)):\n",
    "                norm_dataset = scaler.transform(dataset_cpy[i])\n",
    "                norm_dataset = pd.DataFrame(norm_dataset, columns = columns)\n",
    "                dataset_cpy[i] = norm_dataset\n",
    "            \n",
    "    return scaler, dataset_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7624d4",
   "metadata": {},
   "source": [
    "##### **Denormalize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ac93781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(dataset, scaler = None, normalization_ranges = None):\n",
    "    \"\"\"\n",
    "        Function to denormalize the dataset using the scaler used to normalize the dataset.\n",
    "        Manual denormalization can be used for separate testing data denormalization or for denormalization of the whole dataset.\n",
    "    \"\"\"\n",
    "    dataset_cpy = copy.deepcopy(dataset)\n",
    "    \n",
    "    #######\n",
    "    if scaler is None and normalization_ranges is not None:\n",
    "        X_min = normalization_ranges[\"min\"]\n",
    "        X_max = normalization_ranges[\"max\"]\n",
    "        \n",
    "        dataset_cpy = [arr * (X_max - X_min) + X_min for arr in dataset]\n",
    "            \n",
    "    if scaler is not None:\n",
    "        for item in range(len(dataset)):\n",
    "            dataset_cpy[item] = scaler.inverse_transform(dataset_cpy[item])\n",
    "       \n",
    "    return dataset_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224be7d",
   "metadata": {},
   "source": [
    "##### **Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b38a12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reshape(X, Y, \n",
    "                seq_len,\n",
    "                num_feats, \n",
    "                num_outputs,\n",
    "                batch_size):\n",
    "    \"\"\"\n",
    "        Function to split the data into training and testing sets and reshape the data into the required shape for the LSTM model\n",
    "        Returns a dictionary with the following:\n",
    "            X_train: Training data for the input features\n",
    "            X_test: Testing data for the input features\n",
    "            Y_train: Training data for the output features\n",
    "            Y_test: Testing data for the output features    \n",
    "    \"\"\"\n",
    "    \n",
    "    valid_rows = X.shape[0] // seq_len * seq_len\n",
    "\n",
    "    if len(Y.shape) == 1:\n",
    "        X = X[:valid_rows]\n",
    "        Y = Y[:valid_rows]\n",
    "    else:\n",
    "        X = X[:valid_rows, :]\n",
    "        Y = Y[:valid_rows, :]\n",
    "\n",
    "    num_sequences = X.shape[0] // seq_len\n",
    "       \n",
    "    X = X.reshape(num_sequences, seq_len, num_feats)\n",
    "    Y = Y.reshape(num_sequences, seq_len, num_outputs)\n",
    "              \n",
    "    X_train = X[0: X.shape[0] - (X.shape[0] % batch_size)]\n",
    "    Y_train = Y[0: Y.shape[0] - (Y.shape[0] % batch_size)]\n",
    "       \n",
    "    data = {\"X_train\": X_train, \"Y_train\": Y_train}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad03fae",
   "metadata": {},
   "source": [
    "##### **Train Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b2ba952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_preparation(X, Y,\n",
    "                            num_of_traj, \n",
    "                            BATCH_SIZE,\n",
    "                            TESTING_SIZE,\n",
    "                            SEQ_LEN,\n",
    "                            NUM_FEATS,\n",
    "                            NUM_OUTPUTS,\n",
    "                            ):\n",
    "    \"\"\"\n",
    "        Train data preparation\n",
    "        Return a numpy array of all trajectories, where each trajectory is padded with zeroes to a multiple of the SL.\n",
    "        After reshaping, each trajectory is a single numpy array of size (NUMBER OF SEQUENCES, SEQ_LEN, NUM_FEATS)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to NP Array and get the sequence lengths\n",
    "    training_size = int(num_of_traj * (1 - TESTING_SIZE))\n",
    "    train_traj_seq_lengths = [0.0] * training_size\n",
    "\n",
    "    X_concatenated, Y_concatenated = [], []\n",
    "\n",
    "    for i in range(training_size):\n",
    "        X[i] = np.array(X[i])\n",
    "        Y[i] = np.array(Y[i])\n",
    "\n",
    "        # For data reshaping later on\n",
    "        reminder = X[i].shape[0] % SEQ_LEN\n",
    "        \n",
    "        # Trim trajectories to seq_len and create a single numpy array with all trajectories\n",
    "        if (X[i].shape[0] >= SEQ_LEN):\n",
    "            X[i] = X[i][0 : (X[i].shape[0] - reminder), :]\n",
    "            Y[i] = Y[i][0 : (Y[i].shape[0] - reminder), :]\n",
    "\n",
    "            X_concatenated.extend(X[i])\n",
    "            Y_concatenated.extend(Y[i])\n",
    "        \n",
    "            train_traj_seq_lengths[i] = X[i].shape[0]\n",
    "            \n",
    "    X_new, Y_new = np.array(X_concatenated), np.array(Y_concatenated)\n",
    "\n",
    "    lstm_data = train_reshape(X_new, Y_new, SEQ_LEN, NUM_FEATS, NUM_OUTPUTS, BATCH_SIZE)\n",
    "\n",
    "    X_train = lstm_data[\"X_train\"]\n",
    "    Y_train = lstm_data[\"Y_train\"]\n",
    "    \n",
    "    return X_train, Y_train, training_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc76910",
   "metadata": {},
   "source": [
    "##### **Test Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3960f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_preparation(TRAINING_TESTING_SAME_FILE, \n",
    "                          num_of_traj,\n",
    "                          training_size,\n",
    "                          SEQ_LEN,\n",
    "                          NUM_FEATS,\n",
    "                          TESTING_FILE,\n",
    "                          data,\n",
    "                          X = None, Y = None,\n",
    "                          ):\n",
    "    \n",
    "    \"\"\"\n",
    "        Test data preparation\n",
    "        Returns a list of numpy arrays, where each array is a single trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    if TRAINING_TESTING_SAME_FILE:\n",
    "        X_test = [0.0] * (num_of_traj - training_size)\n",
    "        Y_test = [0.0] * (num_of_traj - training_size)\n",
    "\n",
    "        test_traj_seq_lengths = [0.0] * (num_of_traj - training_size + 1) \n",
    "\n",
    "        idx = 0\n",
    "        for i in range(training_size, num_of_traj):\n",
    "            X[i] = np.array(X[i])\n",
    "            Y[i] = np.array(Y[i])\n",
    "            \n",
    "            test_traj_seq_lengths[idx] = X[i].shape[0]\n",
    "            \n",
    "            # For data reshaping later on\n",
    "            seq_multiplier = X[i].shape[0] // SEQ_LEN\n",
    "            padding_size = (seq_multiplier + 1) * SEQ_LEN - X[i].shape[0]\n",
    "            \n",
    "            padding = np.zeros([ padding_size, NUM_FEATS ])\n",
    "            \n",
    "            X_test[idx] = np.vstack((X[i], padding))\n",
    "            Y_test[idx] = Y[i]\n",
    "            \n",
    "            idx += 1\n",
    "    else:\n",
    "        # In case the testing data is obtained from a different file:\n",
    "        data_test = load_data_from_pickle(TESTING_FILE)\n",
    "        \n",
    "        X_test = [0.0] * len(data_test)\n",
    "        Y_test = [0.0] * len(data_test)\n",
    "        \n",
    "        # Get trajectories min and max values\n",
    "        num_of_traj_test = len(data_test)\n",
    "\n",
    "        test_traj_seq_lengths = [0.0] * num_of_traj_test\n",
    "        \n",
    "        data_test = [data_test[i][COLUMNS] for i in range(num_of_traj_test)]\n",
    "\n",
    "        scaler, data_test = normalize_trajectory_data(dataset = data_test, normalization_type = 'min-max', testing_data_norm=True, scaler=scaler)\n",
    "\n",
    "        X_t, Y_t = [0.0] * num_of_traj_test, [0.0] * num_of_traj_test\n",
    "\n",
    "        for i in range(num_of_traj_test):\n",
    "            # X is composed of the trajectory data starting from the first point to the second last point\n",
    "            X_t[i] =  data[i][COLUMNS].iloc[0:-1] \n",
    "            X_t[i].columns = COLUMNS\n",
    "\n",
    "            # Y is composed of the trajectory data starting from the second point to the last point\n",
    "            Y_t[i] =  data[i][COLUMNS].iloc[1:] \n",
    "            Y_t[i].columns = COLUMNS\n",
    "\n",
    "        for i in range(num_of_traj_test):\n",
    "            X_t[i] = np.array(X_t[i])\n",
    "            Y_t[i] = np.array(Y_t[i])\n",
    "\n",
    "            test_traj_seq_lengths[i] = X[i].shape[0]\n",
    "            \n",
    "            # For data reshaping later on\n",
    "            seq_multiplier = X_t[i].shape[0] // SEQ_LEN\n",
    "            padding_size = (seq_multiplier + 1) * SEQ_LEN - X_t[i].shape[0]\n",
    "            \n",
    "            padding = np.zeros([ padding_size, NUM_FEATS ])\n",
    "            \n",
    "            X_test[i] = np.vstack((X_t[i], padding))\n",
    "            Y_test[i] = Y_t[i]\n",
    "            \n",
    "    return X_test, Y_test, test_traj_seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acabd9",
   "metadata": {},
   "source": [
    "##### **Distance and Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79af3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "        Compute great-circle (Haversine) distance between two lat/lon points in km.\n",
    "        Returns the distance in km.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do this check if one argument is NaN\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return 0 # \n",
    "        \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    return haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * 6371  #Earth radius in km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f73fb001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance_tdrive(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "        Compute great-circle (Haversine) distance between two lat/lon points in km.\n",
    "        Returns the distance in km.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do this check if one argument is NaN\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return np.nan # \n",
    "        \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    return haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * 6371008.7714  #Earth radius in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b65aa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_metrics(Y_test, Y_pred):\n",
    "    \"\"\"\n",
    "        Compute prediction metrics for the model\n",
    "        Returns a dictionary with the following metrics:\n",
    "        - MSE: Mean Squared Error\n",
    "        - KLD: Symetric Kullback-Leibler Divergence (Jeffreys divergence)\n",
    "        - ED: Energy Distance\n",
    "        - WD: Wasserstein Distance\n",
    "    \"\"\"\n",
    "    \n",
    "    # MSE, KLD, ED, WD, HAVERSINE (HS)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    # kld = keras.losses.KLDivergence(Y_test, Y_pred)\n",
    "    kld = 0\n",
    "    ed = float(energy_distance(Y_test, Y_pred))\n",
    "    wd = float(wasserstein_distance(Y_test, Y_pred))\n",
    "    \n",
    "    results = {\"MSE\" : mse, \"KLD\" : kld, \"ED\" : ed, \"WD\" : wd}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dea302ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_metrics(Y_test, Y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute prediction metrics for the model\n",
    "        Returns a dictionary with the following metrics:\n",
    "        - IE: Individual Error\n",
    "        - ISE: Individual Squared Error\n",
    "        - MSE: Mean Squared Error\n",
    "        - ED: Energy Distance, averaged per features\n",
    "    \"\"\"\n",
    "    \n",
    "    errors = [0.0] * len(Y_test)\n",
    "    squared_errors = [0.0] * len(Y_test)\n",
    "    mses = [0.0] * len(Y_test)\n",
    "    eds = [0.0] * len(Y_test)\n",
    "    \n",
    "    for i in range(len(Y_test)):\n",
    "        \n",
    "        err = Y_test[i] - Y_pred[i]\n",
    "        squared_errors[i] = err**2\n",
    "        errors[i] = err\n",
    "        mses[i] = np.mean(err**2)\n",
    "        \n",
    "        eds1= float(energy_distance(Y_test[i][:,0], Y_pred[i][:,0]))\n",
    "        eds2 = float(energy_distance(Y_test[i][:,1], Y_pred[i][:,1]))\n",
    "        eds[i] = np.mean([eds1, eds2])\n",
    "        \n",
    "    results = {\"IE\" : errors, \"ISE\" : squared_errors, \"MSE\" : mses, \"ED\" : eds}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c625440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_point_to_point_haversine_distances(traj1, traj2):\n",
    "    \"\"\"\n",
    "    Compute the haversine point-to-point distance in meters between two trajectories.\n",
    "    \n",
    "    Parameters:\n",
    "        traj1 (array-like): First trajectory as a list or array of [latitude, longitude] pairs.\n",
    "        traj2 (array-like): Second trajectory as a list or array of [latitude, longitude] pairs.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of distances in meters between corresponding points in the two trajectories.\n",
    "    \"\"\"\n",
    "    if len(traj1) != len(traj2):\n",
    "        raise ValueError(\"Trajectories must have the same number of points.\")\n",
    "    \n",
    "    distances = []\n",
    "    for (lat1, lon1), (lat2, lon2) in zip(traj1, traj2):\n",
    "        # Convert degrees to radians\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        # Compute haversine distance in kilometers and convert to meters\n",
    "        distance = haversine_distances([[lat1, lon1], [lat2, lon2]])[0, 1] * 6371000  # Earth radius in meters\n",
    "        distances.append(distance)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2bc673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_loss(y_true, y_pred):\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    DEG2RAD = math.pi / 180.0\n",
    "\n",
    "    lat1, lon1 = tf.unstack(y_true, axis=-1)\n",
    "    lat2, lon2 = tf.unstack(y_pred, axis=-1)\n",
    "\n",
    "    lat1 = lat1 * DEG2RAD\n",
    "    lon1 = lon1 * DEG2RAD\n",
    "    lat2 = lat2 * DEG2RAD\n",
    "    lon2 = lon2 * DEG2RAD\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = tf.sin(dlat / 2.0)**2 + tf.cos(lat1) * tf.cos(lat2) * tf.sin(dlon / 2.0)**2\n",
    "    c = 2.0 * tf.atan2(tf.sqrt(a), tf.sqrt(1.0 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return tf.reduce_mean(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469b2f0",
   "metadata": {},
   "source": [
    "##### **Trajectory Data Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca534336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_act_pred_traj(predicted, \n",
    "                       actual, \n",
    "                       seq_len, \n",
    "                       scatter = False, \n",
    "                       trim_trajectory = False, \n",
    "                       k=1, \n",
    "                       show = True,\n",
    "                       x_range = None,\n",
    "                       y_range = None):\n",
    "    \"\"\"\n",
    "        Plots the actual and predicted trajectories.\n",
    "        Set scatter to True to plot the trajectories as scatter plots.\n",
    "        Set trim_trajectory to True to plot the trajectories with the same length.\n",
    "        Set show to False to not show the plot.\n",
    "        Saves the plot to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not trim_trajectory:\n",
    "        LEN = seq_len\n",
    "        \n",
    "    fig_lons_max = max(actual.lons.max(), predicted.lons.max())\n",
    "    fig_lons_min = min(actual.lons.min(), predicted.lons.min())\n",
    "\n",
    "    fig_lats_max = max(actual.lats.max(), predicted.lats.max())\n",
    "    fig_lats_min = min(actual.lats.min(), predicted.lats.min())\n",
    "    \n",
    "    plt.figure()\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.xlim([fig_lons_min, fig_lons_max])\n",
    "    plt.ylim([fig_lats_min, fig_lats_max])\n",
    "    \n",
    "    for act, pred in zip(actual.groupby(\"id\"), predicted.groupby(\"id\")):\n",
    "                   \n",
    "        act_x, act_y = list(act[1].lons), list(act[1].lats) \n",
    "        \n",
    "        if trim_trajectory:\n",
    "            LEN = list(act[1].length)[0]\n",
    "        \n",
    "        if not scatter:\n",
    "            plt.plot(act_x[0:LEN], act_y[0:LEN], marker=\"None\", linestyle=\"-\", linewidth=0.35, color=\"black\")\n",
    "        else:\n",
    "            plt.scatter(act_x[0:LEN], act_y[0:LEN],  linewidth=0.35, color=\"black\")\n",
    "        \n",
    "        pred_x, pred_y = list(pred[1].lons), list(pred[1].lats)\n",
    "        \n",
    "        if not scatter:\n",
    "            plt.plot(pred_x[0:LEN], pred_y[0:LEN], marker=\"None\", linestyle=\"-\", linewidth=0.35, color=\"red\")\n",
    "        else:\n",
    "            plt.scatter(pred_x[0:LEN], pred_y[0:LEN],  linewidth=0.35, color=\"red\", alpha=0.5)\n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.title(\"Actual vs. Predicted Trajectory for k = \" + str(k))    \n",
    "    plt.legend([\"Actual\", \"Predicted\"])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/Predictions\" + str(k) + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ece8ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_act_pred_traj_one_by_one(predicted, \n",
    "                                    actual, \n",
    "                                    seq_len, \n",
    "                                    scatter = True, \n",
    "                                    trim_trajectory = False, \n",
    "                                    k=1, \n",
    "                                    show = True, \n",
    "                                    num_of_traj_to_plot = 20, \n",
    "                                    start_traj = 0,\n",
    "                                    end_traj = 0,\n",
    "                                    x_range = None,\n",
    "                                    y_range = None,\n",
    "                                    range = None,\n",
    "                                    path = None):\n",
    "    \"\"\"\n",
    "        Plots the actual and predicted trajectories.\n",
    "        Set scatter to True to plot the trajectories as scatter plots.\n",
    "        Set trim_trajectory to True to plot the trajectories with the same length.\n",
    "        Set show to False to not show the plot.\n",
    "        Saves the plot to a file.\n",
    "    \"\"\"\n",
    "           \n",
    "    # fig_lons_max = max(actual.lons.max(), predicted.lons.max())\n",
    "    # fig_lons_min = min(actual.lons.min(), predicted.lons.min())\n",
    "\n",
    "    # fig_lats_max = max(actual.lats.max(), predicted.lats.max())\n",
    "    # fig_lats_min = min(actual.lats.min(), predicted.lats.min())\n",
    "    errors_x = []\n",
    "    errors_y = []\n",
    "    \n",
    "    plt.figure()\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "    \n",
    "    if x_range is not None:\n",
    "        plt.xlim(x_range)\n",
    "        \n",
    "    if y_range is not None:\n",
    "        plt.ylim(y_range)\n",
    "    \n",
    "    # plt.xlim([fig_lons_min, fig_lons_max])\n",
    "    # plt.ylim([fig_lats_min, fig_lats_max])\n",
    "    \n",
    "    index = 0\n",
    "    size = 50\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        \n",
    "        if (end_traj != -1 and index >= start_traj and index <= end_traj) or (range is not None and index in range):\n",
    "                \n",
    "            act_x, act_y = act[:, 0], act[:, 1] \n",
    "            \n",
    "            # Plot the trajectory\n",
    "            if not scatter:\n",
    "                plt.plot(act_x, act_y, marker=\"None\", linestyle=\"-\", linewidth=0.35, color=\"black\")\n",
    "            else:\n",
    "                plt.scatter(act_x, act_y,  s = 50, linewidth=0.35, color=\"black\")\n",
    "            \n",
    "            # Plot the starting and ending point of the trajectory\n",
    "            # Dont add it to the legend\n",
    "            \n",
    "            plt.scatter(act_x[0], act_y[0],  s = size, linewidth=0.35, color=\"black\")\n",
    "            plt.scatter(act_x[-1], act_y[-1],  s = size, linewidth=0.35, color=\"black\", alpha=0.5)\n",
    "            \n",
    "            pred_x, pred_y = pred[:, 0], pred[:, 1]\n",
    "            \n",
    "            if not scatter:\n",
    "                plt.plot(pred_x, pred_y, marker=\"None\", linestyle=\"-\", linewidth=0.35, color=\"red\")\n",
    "            else:\n",
    "                plt.scatter(pred_x, pred_y, s = 50, linewidth=0.35, color=\"red\", alpha=0.5)\n",
    "            \n",
    "            # Plot the starting and ending point of the trajectory\n",
    "            plt.scatter(pred_x[0], pred_y[0],  s = size, linewidth=0.35, color=\"red\")\n",
    "            plt.scatter(pred_x[-1], pred_y[-1],  s = size, linewidth=0.35, color=\"red\", alpha=0.5)\n",
    "            \n",
    "            errors_x.append(act_x - pred_x)\n",
    "            errors_y.append(act_y - pred_y)\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.title(\"Actual vs. Predicted Trajectory\")    \n",
    "    plt.legend([\"Actual\", \"Start point\", \"End point\", \"Predicted\" ])\n",
    "    # plt.show()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if path is not None:\n",
    "        plt.savefig(path + \"Predictions\" + str(k) + \".pdf\")\n",
    "    else:\n",
    "        plt.savefig(\"plots/Predictions\" + str(k) + \".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6048e6",
   "metadata": {},
   "source": [
    "##### **Create LSTM and Recurrent Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "daea19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(LSTM_cells, \n",
    "                      seq_len, \n",
    "                      num_feat,\n",
    "                      batch_size,\n",
    "                      stateful,\n",
    "                      return_seq,\n",
    "                      num_outputs,\n",
    "                      LR,\n",
    "                      SEED,\n",
    "                      ragged = False):\n",
    "    \"\"\"\n",
    "        Create an LSTM model with the specified parameters\n",
    "        Returns the untrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    keras.utils.set_random_seed(SEED)\n",
    "\n",
    "    # In newer versions of Keras, for stateful LSTM, you need to specify the batch_input_shape as the first layer (input layer)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ragged tensor for variable length sequences\n",
    "    if ragged is False:\n",
    "        model.add(keras.layers.InputLayer(batch_input_shape=(batch_size, seq_len, num_feat)))\n",
    "        model.add(keras.layers.LSTM(LSTM_cells, return_sequences = return_seq, stateful = stateful))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(shape=[None, num_feat], batch_size = batch_size, dtype=tf.float32, ragged = True))\n",
    "        model.add(keras.layers.LSTM(LSTM_cells, return_sequences = return_seq, stateful = False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs))\n",
    "    \n",
    "    \n",
    "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = LR,\n",
    "        decay_steps = 50,\n",
    "        decay_rate = 0.99) \n",
    "    \n",
    "    model.compile(optimizer = \"adam\", \n",
    "                  loss = \"mse\", \n",
    "                  #loss = haversine_loss,\n",
    "                  #metrics = [\"mse\", \"mae\", \"mape\", \"kl_divergence\"])\n",
    "                  metrics = [\"mse\"])\n",
    "    \n",
    "    # https://keras.io/api/optimizers/\n",
    "    model.optimizer.lr=lr_schedule\n",
    "    # mdl.optimizer.momentum = 0.99\n",
    "    # mdl.optimizer.use_ema = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ac7efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(RNN_cells, \n",
    "                      seq_len, \n",
    "                      num_feat,\n",
    "                      batch_size,\n",
    "                      stateful,\n",
    "                      return_seq,\n",
    "                      num_outputs,\n",
    "                      LR,\n",
    "                      SEED,\n",
    "                      ragged = False):\n",
    "    \"\"\"\n",
    "        Create an LSTM model with the specified parameters\n",
    "        Returns the untrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    keras.utils.set_random_seed(SEED)\n",
    "\n",
    "    # In newer versions of Keras, for stateful LSTM, you need to specify the batch_input_shape as the first layer (input layer)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ragged tensor for variable length sequences\n",
    "    if ragged is False:\n",
    "        model.add(keras.layers.InputLayer(batch_input_shape=(batch_size, seq_len, num_feat)))\n",
    "        model.add(keras.layers.SimpleRNN(RNN_cells, return_sequences = return_seq, stateful = stateful))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(shape=[None, num_feat], batch_size = batch_size, dtype=tf.float32, ragged = True))\n",
    "        model.add(keras.layers.SimpleRNN(RNN_cells, return_sequences = return_seq, stateful = False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs))\n",
    "    \n",
    "    \n",
    "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = LR,\n",
    "        decay_steps = 40,\n",
    "        decay_rate = 0.96) \n",
    "    \n",
    "    model.compile(optimizer = \"adam\", \n",
    "                  loss = \"mse\", \n",
    "                  metrics = [\"mse\", \"mae\", \"mape\", \"kl_divergence\"])\n",
    "    \n",
    "    # https://keras.io/api/optimizers/\n",
    "    model.optimizer.lr=lr_schedule\n",
    "    # mdl.optimizer.momentum = 0.99\n",
    "    # mdl.optimizer.use_ema = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b5542ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GRU_model(GRU_cells, \n",
    "                      seq_len, \n",
    "                      num_feat,\n",
    "                      batch_size,\n",
    "                      stateful,\n",
    "                      return_seq,\n",
    "                      num_outputs,\n",
    "                      LR,\n",
    "                      SEED,\n",
    "                      ragged = False):\n",
    "    \"\"\"\n",
    "        Create an GRU model with the specified parameters\n",
    "        Returns the untrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    keras.utils.set_random_seed(SEED)\n",
    "\n",
    "    # In newer versions of Keras, for stateful LSTM, you need to specify the batch_input_shape as the first layer (input layer)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Ragged tensor for variable length sequences\n",
    "    if ragged is False:\n",
    "        model.add(keras.layers.InputLayer(batch_input_shape=(batch_size, seq_len, num_feat)))\n",
    "        model.add(keras.layers.GRU(GRU_cells, return_sequences = return_seq, stateful = stateful))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(shape=[None, num_feat], batch_size = batch_size, dtype=tf.float32, ragged = True))\n",
    "        model.add(keras.layers.GRU(GRU_cells, return_sequences = return_seq, stateful = False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs))\n",
    "    \n",
    "    \n",
    "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = LR,\n",
    "        decay_steps = 40,\n",
    "        decay_rate = 0.96) \n",
    "    \n",
    "    model.compile(optimizer = \"adam\", \n",
    "                  loss = \"mse\", \n",
    "                  metrics = [\"mse\", \"mae\", \"mape\", \"kl_divergence\"])\n",
    "    \n",
    "    # https://keras.io/api/optimizers/\n",
    "    model.optimizer.lr=lr_schedule\n",
    "    # mdl.optimizer.momentum = 0.99\n",
    "    # mdl.optimizer.use_ema = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb322a66",
   "metadata": {},
   "source": [
    "##### **Train LSTM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9e6fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras training loop\n",
    "def train_model(model, \n",
    "                X_train, Y_train, \n",
    "                epochs,\n",
    "                batch_size):\n",
    "    \"\"\"\n",
    "        Train the LSTM model with the default keras method\n",
    "        Returns the training history and the trained model\n",
    "    \"\"\"\n",
    "    history = model.fit(X_train, \n",
    "                        Y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs, \n",
    "                        verbose = 0, \n",
    "                        shuffle = False)\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a77df1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop\n",
    "def train_model_manual_loop(model, \n",
    "                            epochs,\n",
    "                            LR,\n",
    "                            batch_size,\n",
    "                            X_train = None,\n",
    "                            Y_train = None,\n",
    "                            reset_states = True):\n",
    "    \"\"\"\n",
    "        Manual training loop for the LSTM model\n",
    "        Works with the model created with the create_LSTM_model function\n",
    "        Works for any batch size\n",
    "        Returns the training history and the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        for step in range(0, len(X_train), batch_size):\n",
    "\n",
    "            x_batch = X_train[step : step + batch_size]  \n",
    "            y_batch = Y_train[step : step + batch_size]  \n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "            # Get gradients\n",
    "            trainable_vars = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "            # Apply gradients to update model weights\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "            # Reset states after batch\n",
    "            if reset_states:\n",
    "                model.reset_states()\n",
    "            \n",
    "    return model.history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a90db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop with sequence resets\n",
    "def train_model_train_on_batch(model, \n",
    "                                X, Y, \n",
    "                                batch_size , \n",
    "                                epochs):\n",
    "    \"\"\"\n",
    "        Manual training loop with sequence resets and train on batch\n",
    "        Works for stateful and stateless models\n",
    "        Works for any batch size\n",
    "        Returns the trained model\n",
    "    \"\"\"\n",
    "    # Manual training loop with sequence resets used with train_on_baatch\n",
    "    for epoch in range(epochs):\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            model.train_on_batch(X[start : start + batch_size], Y[start : start + batch_size])\n",
    "            model.reset_states()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c9ab9",
   "metadata": {},
   "source": [
    "##### **Test LSTM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbe47d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, \n",
    "               X_test, \n",
    "               Y_test, \n",
    "               batch_size):\n",
    "    \"\"\"\n",
    "        Works for both stateful and non-stateful models\n",
    "        Works for any batch size\n",
    "        Returns the predictions of the model on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = model.predict(X_test, batch_size = batch_size, verbose = 0).reshape(-1, 1).flatten()\n",
    "    Y_test = Y_test.reshape(-1, 1).flatten()\n",
    "    \n",
    "    return Y_test, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8140f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_per_trajectory(mdl, \n",
    "                            X_t, \n",
    "                            test_traj_seq_lengths,\n",
    "                            SEQ_LENGTH,\n",
    "                            NUM_FEATS\n",
    "                            ):\n",
    "    \"\"\"\n",
    "        Works for both stateful and non-stateful models\n",
    "        Works for any batch size\n",
    "        Returns the predictions of the model on the test data\n",
    "    \"\"\"\n",
    "    mdl.layers[0].reset_states()\n",
    "    \n",
    "    Y_preds = [0.0] * len(X_t)\n",
    "\n",
    "    for i in range(len(X_t)):\n",
    "        X_t[i] = X_t[i].reshape(-1, SEQ_LENGTH, NUM_FEATS)\n",
    "    \n",
    "    for i in range(len(X_t)):\n",
    "        \n",
    "        y_pred = mdl.predict(X_t[i], batch_size = 1, verbose = 0).reshape(-1, NUM_FEATS)\n",
    "        y_pred = y_pred[0:test_traj_seq_lengths[i], :]\n",
    "        Y_preds[i] = y_pred\n",
    "        \n",
    "        mdl.layers[0].reset_states()\n",
    "        \n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b51f7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_all_trajectories(model, \n",
    "                                X_test, \n",
    "                                Y_test, \n",
    "                                batch_size):\n",
    "    \"\"\"\n",
    "        Works for both stateful and non-stateful models\n",
    "        Works for any batch size\n",
    "        Returns the predictions of the model on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = model.predict(X_test, batch_size = batch_size).reshape(-1, 1).flatten()\n",
    "    Y_test = Y_test.reshape(-1, 1).flatten()\n",
    "   \n",
    "    return Y_test, Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86751d",
   "metadata": {},
   "source": [
    "**General Test with Dummy Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f38b3f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'true'\n"
     ]
    }
   ],
   "source": [
    "%%script true --no-raise-error\n",
    "%%cache \n",
    "\n",
    "data = load_dummy_data()\n",
    "scaler,data = normalize_data(dataset = data, normalization_type = 'min-max')\n",
    "\n",
    "feature_cols = 'Temperature'\n",
    "target_col = 'Zone 1 Power Consumption'\n",
    "\n",
    "X = data[feature_cols].values\n",
    "Y = data[target_col].values\n",
    "\n",
    "X = X[0:5000]\n",
    "Y = Y[0:5000]\n",
    "\n",
    "data = train_test_split_fun(X = X, Y = Y,\n",
    "                            testing_size = 0.2, \n",
    "                            seq_len = SEQ_LEN, \n",
    "                            num_feats = NUM_FEATS, \n",
    "                            num_outputs = NUM_OUTPUTS, \n",
    "                            batch_size = BATCH_SIZE)\n",
    "\n",
    "X_train = data[\"X_train\"]\n",
    "Y_train = data[\"Y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "Y_test = data[\"Y_test\"]\n",
    "\n",
    "model = create_LSTM_model(LSTM_cells = LSTM_CELLS,\n",
    "                          seq_len = SEQ_LEN,\n",
    "                          num_feat = NUM_FEATS,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          stateful = STATEFUL,\n",
    "                          return_seq = RETURN_SEQ,\n",
    "                          num_outputs = NUM_OUTPUTS,\n",
    "                          LR = LR,\n",
    "                          SEED = SEED)\n",
    "\n",
    "history, model = train_model(model = model,\n",
    "                    X_train = X_train,\n",
    "                    Y_train = Y_train,\n",
    "                    epochs = EPOCHS,\n",
    "                    batch_size = BATCH_SIZE)\n",
    "\n",
    "Y_test, Y_pred = test_model(model,\n",
    "                            X_test = X_test,\n",
    "                            Y_test = Y_test,\n",
    "                            batch_size = BATCH_SIZE)\n",
    "\n",
    "results = compute_prediction_metrics(Y_test = Y_test, Y_pred = Y_pred)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162dbd96",
   "metadata": {},
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3b316",
   "metadata": {},
   "source": [
    "#### **Load Trajectory Data Once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bcb267bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trajectory data for faster execution\n",
    "data = load_data_from_pickle(DATASET[SELECTED_DATASET], TOTAL_TRAJS)\n",
    "\n",
    "# Get the data in the selected square\n",
    "data = get_data_in_square(data = data, square = DATA_SQUARE[SELECTED_DATASET])\n",
    "\n",
    "# Get trajectories min and max values\n",
    "mins, maxs =  get_min_max_from_data(data)\n",
    "\n",
    "# Get number of trajectories\n",
    "num_of_traj = len(data)\n",
    "\n",
    "# Normalize the data using the min and max values\n",
    "normalization_ranges = {\"min\": mins, \"max\": maxs}\n",
    "\n",
    "# Only keep the lat and lon columns for now\n",
    "data = [data[i][COLUMNS] for i in range(num_of_traj)]\n",
    "\n",
    "# Normalize the data using scaler or normalization ranges\n",
    "scaler, data = normalize_trajectory_data(dataset = data, normalization_type = 'min-max')\n",
    "\n",
    "# Create X and Y from the data\n",
    "X, Y =  create_X_Y_from_data(data, num_of_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126e11d",
   "metadata": {},
   "source": [
    "#### **Create Trajectory Data Training/Testing Splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2cf28b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data Preparation\n",
    "X_train, Y_train, training_size = train_data_preparation(X = copy.deepcopy(X) , Y= copy.deepcopy(Y),\n",
    "                                                        num_of_traj = num_of_traj,\n",
    "                                                        BATCH_SIZE = BATCH_SIZE,\n",
    "                                                        TESTING_SIZE = TESTING_SIZE,\n",
    "                                                        SEQ_LEN = SEQ_LEN,\n",
    "                                                        NUM_FEATS = NUM_FEATS,\n",
    "                                                        NUM_OUTPUTS = NUM_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "387458fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Preparation\n",
    "X_test, Y_test, test_traj_seq_lengths = test_data_preparation(TRAINING_TESTING_SAME_FILE = TRAINING_TESTING_SAME_FILE,\n",
    "                                                                X = copy.deepcopy(X), Y = copy.deepcopy(Y),\n",
    "                                                                num_of_traj = num_of_traj,\n",
    "                                                                training_size = training_size,\n",
    "                                                                SEQ_LEN = SEQ_LEN,\n",
    "                                                                NUM_FEATS = NUM_FEATS,\n",
    "                                                                TESTING_FILE = None,\n",
    "                                                                data = data)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865b35a",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7fe08",
   "metadata": {},
   "source": [
    "##### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "00802a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(X_train, Y_train, X_test, Y_test, hyperparameters, num_of_traj, training_size, SEQ_LEN, NUM_FEATS, NUM_OUTPUTS, BATCH_SIZE, STATEFUL, RETURN_SEQ, LR, EPOCHS):\n",
    "    best_model =  np.inf\n",
    "    best_hyperparameters = None\n",
    "    \n",
    "    for LSTM_cells in hyperparameters[\"LSTM_cells\"]:\n",
    "            for batch_size in hyperparameters[\"batch_size\"]:\n",
    "                for stateful in hyperparameters[\"stateful\"]:\n",
    "                    for lr in hyperparameters[\"LR\"]:\n",
    "                        for epochs in hyperparameters[\"EPOCHS\"]:\n",
    "                            \n",
    "                            # Clear the model and reset \n",
    "                            gc.collect()\n",
    "                            keras.backend.clear_session()   \n",
    "                            \n",
    "                            print(f\"Training LSTM model with {LSTM_cells} LSTM cells, {batch_size} batch size, stateful = {stateful}, LR = {lr}, epochs = {epochs}\")\n",
    "                            \n",
    "                            model = create_LSTM_model(LSTM_cells = LSTM_cells,\n",
    "                                                      seq_len = SEQ_LEN,\n",
    "                                                      num_feat = NUM_FEATS,\n",
    "                                                      batch_size = batch_size,\n",
    "                                                      stateful = stateful,\n",
    "                                                      return_seq = RETURN_SEQ,\n",
    "                                                      num_outputs = NUM_OUTPUTS,\n",
    "                                                      LR = lr,\n",
    "                                                      SEED = SEED,\n",
    "                                                      ragged = False)\n",
    "\n",
    "                            history, model = train_model(model = model,\n",
    "                                                        X_train = X_train,  \n",
    "                                                        Y_train = Y_train,\n",
    "                                                        epochs = epochs,\n",
    "                                                        batch_size = batch_size)    \n",
    "\n",
    "\n",
    "                            model_sl1 = create_LSTM_model(LSTM_cells = LSTM_cells,\n",
    "                                                    seq_len = SEQ_LEN,\n",
    "                                                    num_feat = NUM_FEATS,\n",
    "                                                    batch_size = 1,\n",
    "                                                    stateful = True,\n",
    "                                                    return_seq = RETURN_SEQ,\n",
    "                                                    num_outputs = NUM_OUTPUTS,\n",
    "                                                    LR = lr,\n",
    "                                                    SEED = SEED,\n",
    "                                                    ragged = False)\n",
    "\n",
    "                            # Set weights and states\n",
    "                            model_sl1.set_weights(model.get_weights())\n",
    "                            # model_sl1.layers[0].states = model.layers[0].states\n",
    "                            \n",
    "                            # Select best model based on testing loss\n",
    "                            Y_pred = test_model_per_trajectory(mdl=model_sl1, \n",
    "                                                                X_t = X_test,\n",
    "                                                                test_traj_seq_lengths = test_traj_seq_lengths,\n",
    "                                                                SEQ_LENGTH=SEQ_LEN,\n",
    "                                                                NUM_FEATS=NUM_FEATS)\n",
    "\n",
    "                            results = compute_trajectory_metrics(Y_test = Y_test, Y_pred = Y_pred)\n",
    "                            \n",
    "                            loss = np.mean(results[\"MSE\"])\n",
    "                                \n",
    "                            if  loss <= best_model:\n",
    "                                best_hyperparameters = {\n",
    "                                    \"LSTM_cells\": LSTM_cells,\n",
    "                                    \"seq_len\": SEQ_LEN,\n",
    "                                    \"batch_size\": batch_size,\n",
    "                                    \"stateful\": stateful,\n",
    "                                    \"LR\": lr,\n",
    "                                    \"EPOCHS\": epochs   \n",
    "                                }\n",
    "                                print(\"Best Hyperparameters: \", best_hyperparameters)\n",
    "                                best_model = loss\n",
    "                                best_mod = model\n",
    "                                print(\"Best model found with loss: \", best_model)\n",
    "                                \n",
    "    return best_mod, best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fe928a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.0001, epochs = 25\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.0001, 'EPOCHS': 25}\n",
      "Best model found with loss:  0.0019075744808119527\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.0001, epochs = 50\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.0001, 'EPOCHS': 50}\n",
      "Best model found with loss:  0.0005226816937100253\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.0001, epochs = 75\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.0001, 'EPOCHS': 75}\n",
      "Best model found with loss:  0.00047849804271720705\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.0001, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.0001, 'EPOCHS': 100}\n",
      "Best model found with loss:  0.00047832720645233067\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.001, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.001, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.001, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.001, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.001, 'EPOCHS': 100}\n",
      "Best model found with loss:  0.00047832720645233067\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.01, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.01, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.01, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.01, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.01, 'EPOCHS': 100}\n",
      "Best model found with loss:  0.00047832720645233067\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.1, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.1, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.1, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = True, LR = 0.1, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': True, 'LR': 0.1, 'EPOCHS': 100}\n",
      "Best model found with loss:  0.00047832720645233067\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.0001, epochs = 25\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.0001, 'EPOCHS': 25}\n",
      "Best model found with loss:  5.9598254589195355e-05\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.0001, epochs = 50\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.0001, 'EPOCHS': 50}\n",
      "Best model found with loss:  1.9488161021454813e-05\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.0001, epochs = 75\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.0001, 'EPOCHS': 75}\n",
      "Best model found with loss:  1.61502965441666e-05\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.0001, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.0001, 'EPOCHS': 100}\n",
      "Best model found with loss:  1.6065638252656055e-05\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.001, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.001, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.001, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.001, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.001, 'EPOCHS': 100}\n",
      "Best model found with loss:  1.6065638252656055e-05\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.01, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.01, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.01, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.01, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.01, 'EPOCHS': 100}\n",
      "Best model found with loss:  1.6065638252656055e-05\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.1, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.1, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.1, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 8 batch size, stateful = False, LR = 0.1, epochs = 100\n",
      "Best Hyperparameters:  {'LSTM_cells': 32, 'seq_len': 25, 'batch_size': 8, 'stateful': False, 'LR': 0.1, 'EPOCHS': 100}\n",
      "Best model found with loss:  1.6065638252656055e-05\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.0001, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.0001, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.0001, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.0001, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.001, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.001, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.001, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.001, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.01, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.01, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.01, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.01, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.1, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.1, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.1, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = True, LR = 0.1, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.0001, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.0001, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.0001, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.0001, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.001, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.001, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.001, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.001, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.01, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.01, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.01, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.01, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.1, epochs = 25\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.1, epochs = 50\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.1, epochs = 75\n",
      "Training LSTM model with 32 LSTM cells, 16 batch size, stateful = False, LR = 0.1, epochs = 100\n",
      "Training LSTM model with 32 LSTM cells, 32 batch size, stateful = True, LR = 0.0001, epochs = 25\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs defined at (most recent call last):\n  File \"C:\\python\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"C:\\python\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\python\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n\n  File \"C:\\python\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n\n  File \"C:\\python\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n\n  File \"C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_3516\\870576581.py\", line 24, in <module>\n\n  File \"C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_3516\\2099767919.py\", line 28, in hyperparameter_optimization\n\n  File \"C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_3516\\1480939710.py\", line 10, in train_model\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 371, in fit\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 219, in function\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 113, in one_step_on_data\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 77, in train_step\n\nIncompatible shapes: [16,25,2] vs. [32,25,2]\n\t [[{{node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs}}]] [Op:__inference_multi_step_on_iterator_7165340]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 24\u001b[0m\n\u001b[0;32m     12\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM_cells\u001b[39m\u001b[38;5;124m\"\u001b[39m: LSTM_CELLS_RANGE,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: SEQ_LEN_RANGE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m: EPOCHS_RANGE,\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Hyperparameter Optimization\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m best_model, best_hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mnum_of_traj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_of_traj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtraining_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mNUM_FEATS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_FEATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mNUM_OUTPUTS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_OUTPUTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mSTATEFUL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSTATEFUL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mRETURN_SEQ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRETURN_SEQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_hyperparameters)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_model\u001b[38;5;241m.\u001b[39msummary())\n",
      "Cell \u001b[1;32mIn[135], line 28\u001b[0m, in \u001b[0;36mhyperparameter_optimization\u001b[1;34m(X_train, Y_train, X_test, Y_test, hyperparameters, num_of_traj, training_size, SEQ_LEN, NUM_FEATS, NUM_OUTPUTS, BATCH_SIZE, STATEFUL, RETURN_SEQ, LR, EPOCHS)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining LSTM model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLSTM_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LSTM cells, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batch size, stateful = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstateful\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epochs = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m create_LSTM_model(LSTM_cells \u001b[38;5;241m=\u001b[39m LSTM_cells,\n\u001b[0;32m     18\u001b[0m                           seq_len \u001b[38;5;241m=\u001b[39m SEQ_LEN,\n\u001b[0;32m     19\u001b[0m                           num_feat \u001b[38;5;241m=\u001b[39m NUM_FEATS,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m                           SEED \u001b[38;5;241m=\u001b[39m SEED,\n\u001b[0;32m     26\u001b[0m                           ragged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m history, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     35\u001b[0m model_sl1 \u001b[38;5;241m=\u001b[39m create_LSTM_model(LSTM_cells \u001b[38;5;241m=\u001b[39m LSTM_cells,\n\u001b[0;32m     36\u001b[0m                         seq_len \u001b[38;5;241m=\u001b[39m SEQ_LEN,\n\u001b[0;32m     37\u001b[0m                         num_feat \u001b[38;5;241m=\u001b[39m NUM_FEATS,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m                         SEED \u001b[38;5;241m=\u001b[39m SEED,\n\u001b[0;32m     44\u001b[0m                         ragged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Set weights and states\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[94], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, Y_train, epochs, batch_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model, \n\u001b[0;32m      3\u001b[0m                 X_train, Y_train, \n\u001b[0;32m      4\u001b[0m                 epochs,\n\u001b[0;32m      5\u001b[0m                 batch_size):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m        Train the LSTM model with the default keras method\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m        Returns the training history and the trained model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history, model\n",
      "File \u001b[1;32md:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs defined at (most recent call last):\n  File \"C:\\python\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"C:\\python\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\python\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n\n  File \"C:\\python\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n\n  File \"C:\\python\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n\n  File \"C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_3516\\870576581.py\", line 24, in <module>\n\n  File \"C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_3516\\2099767919.py\", line 28, in hyperparameter_optimization\n\n  File \"C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_3516\\1480939710.py\", line 10, in train_model\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 371, in fit\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 219, in function\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 113, in one_step_on_data\n\n  File \"d:\\WORK\\laptop\\PHD\\Experimente\\Trajectory\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 77, in train_step\n\nIncompatible shapes: [16,25,2] vs. [32,25,2]\n\t [[{{node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs}}]] [Op:__inference_multi_step_on_iterator_7165340]"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization\n",
    "# Degine dicttionary of hyperparameters with ranges!\n",
    "\n",
    "# Hyperparameter ranges\n",
    "LSTM_CELLS_RANGE = [32, 64, 128]\n",
    "SEQ_LEN_RANGE = [10, 20, 30, 40]\n",
    "BATCH_SIZE_RANGE = [8, 16, 32, 64]\n",
    "STATEFUL_RANGE = [True, False]\n",
    "LR_RANGE = [0.0001, 0.001, 0.01, 0.1]\n",
    "EPOCHS_RANGE = [25, 50, 75, 100]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"LSTM_cells\": LSTM_CELLS_RANGE,\n",
    "    \"seq_len\": SEQ_LEN_RANGE,\n",
    "    \"batch_size\": BATCH_SIZE_RANGE,\n",
    "    \"stateful\": STATEFUL_RANGE,\n",
    "    \"LR\": LR_RANGE,\n",
    "    \"EPOCHS\": EPOCHS_RANGE,\n",
    "}\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "\n",
    "best_model, best_hyperparameters = hyperparameter_optimization(X_train = X_train,\n",
    "                                                                Y_train = Y_train,\n",
    "                                                                X_test = X_test,\n",
    "                                                                Y_test = Y_test,\n",
    "                                                                hyperparameters = hyperparameters,\n",
    "                                                                num_of_traj = num_of_traj,\n",
    "                                                                training_size = training_size,\n",
    "                                                                SEQ_LEN = SEQ_LEN,\n",
    "                                                                NUM_FEATS = NUM_FEATS,\n",
    "                                                                NUM_OUTPUTS = NUM_OUTPUTS,\n",
    "                                                                BATCH_SIZE = BATCH_SIZE,\n",
    "                                                                STATEFUL = STATEFUL,\n",
    "                                                                RETURN_SEQ = RETURN_SEQ,\n",
    "                                                                LR = LR,\n",
    "                                                                EPOCHS = EPOCHS)\n",
    "\n",
    "print(best_hyperparameters)\n",
    "print(best_model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
